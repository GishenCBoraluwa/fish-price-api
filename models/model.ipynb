{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c85dd3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Fish Price Prediction Pipeline\n",
    "==============================================\n",
    "\n",
    "A comprehensive machine learning pipeline for predicting fish prices using multiple algorithms:\n",
    "- Neural Networks (LSTM-based)\n",
    "- Random Forest\n",
    "- XGBoost\n",
    "\n",
    "Author: AI Assistant\n",
    "Version: 1.0.0\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Tuple, Optional, Union, Any\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# Configure warnings and logging\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),\n",
    "        logging.FileHandler('fish_price_pipeline.log')\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Configuration class for fish price prediction pipeline\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Data parameters\n",
    "        self.horizon = 7  # prediction horizon in days\n",
    "        self.sequence_length = 14  # lookback window\n",
    "        self.test_size = 0.2\n",
    "        self.val_size = 0.2\n",
    "        self.random_state = 42\n",
    "\n",
    "        # Features to normalize with StandardScaler (zero mean, unit variance)\n",
    "        self.standardize_features = [\n",
    "            'temperature_2m_mean (°C)', 'wind_speed_10m_max (km/h)',\n",
    "            'wind_gusts_10m_max (km/h)', 'cloud_cover_mean (%)',\n",
    "            'precipitation_sum (mm)', 'relative_humidity_2m_mean (%)',\n",
    "            'wet_bulb_temperature_2m_mean (°C)', 'wind_speed_10m_mean (km/h)',\n",
    "            'wind_gusts_10m_mean (km/h)', 'surface_pressure_mean (hPa)',\n",
    "            'rain_sum (mm)', 'pressure_msl_mean (hPa)',\n",
    "            'shortwave_radiation_sum (MJ/m²)', 'et0_fao_evapotranspiration (mm)',\n",
    "            'wind_direction_10m_dominant (°)', 'sunshine_duration (s)',\n",
    "            'wave_height_max (m)', 'wind_wave_height_max (m)',\n",
    "            'swell_wave_height_max (m)', 'wave_period_max (s)',\n",
    "            'wind_wave_period_max (s)', 'wave_direction_dominant (°)'\n",
    "        ]\n",
    "\n",
    "        # Features to normalize by dividing by maximum value\n",
    "        self.normalize_features = [\n",
    "            'dollar_rate', 'Kerosene (LK)', 'Diesel (LAD)', 'Super Diesel (LSD)'\n",
    "        ]\n",
    "\n",
    "        # Target columns\n",
    "        self.target_columns = ['avg_ws_price', 'avg_rt_price']\n",
    "\n",
    "        # Model parameters\n",
    "        self.model_type = 'random_forest'  # 'neural_network', 'random_forest', or 'xgboost'\n",
    "\n",
    "        # Neural Network parameters\n",
    "        self.hidden_size = 128\n",
    "        self.num_layers = 3\n",
    "        self.dropout = 0.2\n",
    "        self.learning_rate = 0.001\n",
    "        self.batch_size = 32\n",
    "        self.epochs = 100\n",
    "        self.patience = 10\n",
    "\n",
    "        # Random Forest parameters\n",
    "        self.rf_n_estimators = 100\n",
    "        self.rf_max_depth = 20\n",
    "        self.rf_min_samples_split = 5\n",
    "        self.rf_min_samples_leaf = 2\n",
    "\n",
    "        # XGBoost parameters\n",
    "        self.xgb_n_estimators = 100\n",
    "        self.xgb_max_depth = 6\n",
    "        self.xgb_learning_rate = 0.1\n",
    "        self.xgb_subsample = 0.8\n",
    "        self.xgb_colsample_bytree = 0.8\n",
    "        self.xgb_reg_alpha = 0.1\n",
    "        self.xgb_reg_lambda = 1.0\n",
    "        self.xgb_early_stopping_rounds = 10\n",
    "\n",
    "        # Data handling parameters\n",
    "        self.min_non_zero_ratio = 0.1\n",
    "        self.max_date_gap_days = 3\n",
    "\n",
    "    def validate(self) -> bool:\n",
    "        \"\"\"Validate configuration parameters\"\"\"\n",
    "        try:\n",
    "            assert self.model_type in ['neural_network', 'random_forest', 'xgboost']\n",
    "            assert 0 < self.test_size < 1\n",
    "            assert 0 < self.val_size < 1\n",
    "            assert self.test_size + self.val_size < 1\n",
    "            assert self.horizon > 0\n",
    "            assert self.sequence_length > 0\n",
    "            assert len(self.target_columns) > 0\n",
    "            return True\n",
    "        except AssertionError as e:\n",
    "            logger.error(f\"Configuration validation failed: {e}\")\n",
    "            return False\n",
    "\n",
    "\n",
    "class DataValidator:\n",
    "    \"\"\"Data validation utilities\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def validate_dataframe(df: pd.DataFrame, required_columns: List[str]) -> Tuple[bool, List[str]]:\n",
    "        \"\"\"Validate DataFrame structure and content\"\"\"\n",
    "        errors = []\n",
    "\n",
    "        if df is None or df.empty:\n",
    "            errors.append(\"DataFrame is None or empty\")\n",
    "            return False, errors\n",
    "\n",
    "        # Check required columns\n",
    "        missing_cols = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            errors.append(f\"Missing required columns: {missing_cols}\")\n",
    "\n",
    "        # Check for duplicates\n",
    "        if df.duplicated().any():\n",
    "            errors.append(f\"Found {df.duplicated().sum()} duplicate rows\")\n",
    "\n",
    "        # Check data types\n",
    "        if 'Date' in df.columns:\n",
    "            try:\n",
    "                pd.to_datetime(df['Date'])\n",
    "            except Exception:\n",
    "                errors.append(\"Date column cannot be converted to datetime\")\n",
    "\n",
    "        return len(errors) == 0, errors\n",
    "\n",
    "\n",
    "class FishPriceDataProcessor:\n",
    "    \"\"\"Data preprocessing and feature engineering for fish price prediction\"\"\"\n",
    "\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.standard_scaler = StandardScaler()\n",
    "        self.normalizers = {}\n",
    "        self.fish_encoder = LabelEncoder()\n",
    "        self.feature_columns = []\n",
    "        self.is_fitted = False\n",
    "\n",
    "    def validate_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Validate input data and ensure required columns exist\"\"\"\n",
    "        required_cols = ['Date', 'Fish Type'] + self.config.target_columns\n",
    "        is_valid, errors = DataValidator.validate_dataframe(df, required_cols)\n",
    "\n",
    "        if not is_valid:\n",
    "            raise ValueError(f\"Data validation failed: {'; '.join(errors)}\")\n",
    "\n",
    "        df = df.copy()\n",
    "\n",
    "        try:\n",
    "            df['Date'] = pd.to_datetime(df['Date'])\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error converting Date column: {e}\")\n",
    "\n",
    "        if len(df) < self.config.sequence_length + self.config.horizon:\n",
    "            raise ValueError(\n",
    "                f\"Insufficient data: need at least {self.config.sequence_length + self.config.horizon} rows, got {len(df)}\"\n",
    "            )\n",
    "\n",
    "        return df\n",
    "\n",
    "    def create_seasonal_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Create seasonal and temporal features from date\"\"\"\n",
    "        df = df.copy()\n",
    "\n",
    "        try:\n",
    "            # Extract temporal components\n",
    "            df['day_of_year'] = df['Date'].dt.dayofyear\n",
    "            df['week_of_year'] = df['Date'].dt.isocalendar().week.astype(int)\n",
    "            df['month'] = df['Date'].dt.month\n",
    "            df['quarter'] = df['Date'].dt.quarter\n",
    "            df['day_of_week'] = df['Date'].dt.dayofweek\n",
    "\n",
    "            # Create cyclical features to capture seasonal patterns\n",
    "            df['day_of_year_sin'] = np.sin(2 * np.pi * df['day_of_year'] / 365.25)\n",
    "            df['day_of_year_cos'] = np.cos(2 * np.pi * df['day_of_year'] / 365.25)\n",
    "            df['week_of_year_sin'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\n",
    "            df['week_of_year_cos'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\n",
    "            df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "            df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "            df['day_of_week_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "            df['day_of_week_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating seasonal features: {e}\")\n",
    "            raise\n",
    "\n",
    "    def create_historical_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Create historical price features for each fish type\"\"\"\n",
    "        df = df.copy()\n",
    "        df = df.sort_values(['Fish Type', 'Date'])\n",
    "\n",
    "        try:\n",
    "            for fish_type in df['Fish Type'].unique():\n",
    "                fish_mask = df['Fish Type'] == fish_type\n",
    "                fish_data = df[fish_mask].copy()\n",
    "\n",
    "                # Create rolling statistics\n",
    "                for window in [7, 14, 30]:\n",
    "                    for target in self.config.target_columns:\n",
    "                        if target not in fish_data.columns:\n",
    "                            continue\n",
    "\n",
    "                        valid_prices = fish_data[target].replace(0, np.nan)\n",
    "\n",
    "                        # Rolling mean\n",
    "                        rolling_mean = valid_prices.rolling(\n",
    "                            window=window, min_periods=window\n",
    "                        ).mean()\n",
    "                        df.loc[fish_mask, f'{target}_rolling_{window}d'] = rolling_mean\n",
    "\n",
    "                        # Rolling standard deviation\n",
    "                        rolling_std = valid_prices.rolling(\n",
    "                            window=window, min_periods=window\n",
    "                        ).std()\n",
    "                        df.loc[fish_mask, f'{target}_rolling_std_{window}d'] = rolling_std\n",
    "\n",
    "                # Create lag features\n",
    "                for lag in [1, 3, 7]:\n",
    "                    for target in self.config.target_columns:\n",
    "                        if target in fish_data.columns:\n",
    "                            df.loc[fish_mask, f'{target}_lag_{lag}d'] = fish_data[target].shift(lag)\n",
    "\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating historical features: {e}\")\n",
    "            raise\n",
    "\n",
    "    def clean_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Clean and prepare data\"\"\"\n",
    "        df = df.copy()\n",
    "\n",
    "        try:\n",
    "            # Handle target columns\n",
    "            for col in self.config.target_columns:\n",
    "                if col in df.columns:\n",
    "                    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "                    df[col] = df[col].replace(0, np.nan)\n",
    "\n",
    "            # Drop rows where all target columns are NaN\n",
    "            initial_length = len(df)\n",
    "            df = df.dropna(subset=self.config.target_columns, how='all')\n",
    "            logger.info(f\"Removed {initial_length - len(df)} rows with all missing targets\")\n",
    "\n",
    "            # Fill remaining NaN targets with 0\n",
    "            df[self.config.target_columns] = df[self.config.target_columns].fillna(0)\n",
    "\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error cleaning data: {e}\")\n",
    "            raise\n",
    "\n",
    "    def fit_transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Fit transformers and transform the data\"\"\"\n",
    "        try:\n",
    "            # Validate and clean data\n",
    "            df = self.validate_data(df)\n",
    "            df = self.clean_data(df)\n",
    "\n",
    "            # Create features\n",
    "            df = self.create_seasonal_features(df)\n",
    "            df = self.create_historical_features(df)\n",
    "\n",
    "            # Handle NaN values\n",
    "            feature_cols = [col for col in df.columns if any(x in col for x in ['lag', 'rolling'])]\n",
    "            if feature_cols:\n",
    "                for col in feature_cols:\n",
    "                    df[col] = df[col].fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
    "\n",
    "            df = df.fillna(0)\n",
    "\n",
    "            # Encode categorical variables\n",
    "            if 'Fish Type' in df.columns:\n",
    "                df['Fish Type_encoded'] = self.fish_encoder.fit_transform(df['Fish Type'])\n",
    "\n",
    "            # Apply scaling\n",
    "            available_std_features = [col for col in self.config.standardize_features if col in df.columns]\n",
    "            if available_std_features:\n",
    "                df[available_std_features] = self.standard_scaler.fit_transform(df[available_std_features])\n",
    "\n",
    "            # Apply normalization\n",
    "            for feature in self.config.normalize_features:\n",
    "                if feature in df.columns:\n",
    "                    max_val = df[feature].max()\n",
    "                    if max_val == 0 or np.isnan(max_val):\n",
    "                        max_val = 1.0\n",
    "                        logger.warning(f\"Max value for {feature} is 0 or NaN, using 1.0 for normalization\")\n",
    "                    self.normalizers[feature] = max_val\n",
    "                    df[feature] = df[feature] / max_val\n",
    "\n",
    "            # Define feature columns\n",
    "            exclude_cols = [\n",
    "                'Date', 'Fish Type', 'index', 'day_of_year', 'week_of_year',\n",
    "                'month', 'quarter', 'day_of_week'\n",
    "            ] + self.config.target_columns\n",
    "            exclude_cols.extend([col for col in df.columns if col.startswith('Unnamed')])\n",
    "\n",
    "            self.feature_columns = [col for col in df.columns if col not in exclude_cols]\n",
    "            self.is_fitted = True\n",
    "\n",
    "            logger.info(f\"Feature engineering completed. Total features: {len(self.feature_columns)}\")\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in fit_transform: {e}\")\n",
    "            raise\n",
    "\n",
    "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Transform new data using fitted transformers\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Transformers not fitted. Call fit_transform first.\")\n",
    "\n",
    "        try:\n",
    "            df = self.validate_data(df)\n",
    "            df = self.clean_data(df)\n",
    "            df = self.create_seasonal_features(df)\n",
    "            df = self.create_historical_features(df)\n",
    "\n",
    "            # Handle NaN values\n",
    "            feature_cols = [col for col in df.columns if any(x in col for x in ['lag', 'rolling'])]\n",
    "            if feature_cols:\n",
    "                for col in feature_cols:\n",
    "                    df[col] = df[col].fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
    "\n",
    "            df = df.fillna(0)\n",
    "\n",
    "            # Apply transformations\n",
    "            if 'Fish Type' in df.columns:\n",
    "                # Handle unknown fish types\n",
    "                known_types = self.fish_encoder.classes_\n",
    "                df.loc[~df['Fish Type'].isin(known_types), 'Fish Type'] = known_types[0]\n",
    "                df['Fish Type_encoded'] = self.fish_encoder.transform(df['Fish Type'])\n",
    "\n",
    "            # Apply scaling\n",
    "            available_std_features = [col for col in self.config.standardize_features if col in df.columns]\n",
    "            if available_std_features:\n",
    "                df[available_std_features] = self.standard_scaler.transform(df[available_std_features])\n",
    "\n",
    "            # Apply normalization\n",
    "            for feature in self.config.normalize_features:\n",
    "                if feature in df.columns and feature in self.normalizers:\n",
    "                    df[feature] = df[feature] / self.normalizers[feature]\n",
    "\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in transform: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "class FishPriceDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for fish price prediction\"\"\"\n",
    "\n",
    "    def __init__(self, data: pd.DataFrame, processor: FishPriceDataProcessor, config: Config, mode: str = 'train'):\n",
    "        self.data = data.copy().sort_values(['Fish Type', 'Date'])\n",
    "        self.processor = processor\n",
    "        self.config = config\n",
    "        self.mode = mode\n",
    "        self.sequences = []\n",
    "\n",
    "        try:\n",
    "            self.sequences = self._create_sequences()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating sequences: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _create_sequences(self) -> List[Dict[str, np.ndarray]]:\n",
    "        \"\"\"Create sequences for time series prediction\"\"\"\n",
    "        sequences = []\n",
    "\n",
    "        feature_columns = self.processor.feature_columns\n",
    "        if not feature_columns:\n",
    "            raise ValueError(\"No feature columns found\")\n",
    "\n",
    "        missing_features = [col for col in feature_columns if col not in self.data.columns]\n",
    "        if missing_features:\n",
    "            raise ValueError(f\"Missing feature columns: {missing_features}\")\n",
    "\n",
    "        data_reset = self.data.reset_index(drop=True)\n",
    "        feature_data = data_reset[feature_columns].values\n",
    "        target_data = data_reset[self.config.target_columns].values\n",
    "\n",
    "        for fish_type_encoded in data_reset['Fish Type_encoded'].unique():\n",
    "            fish_positions = data_reset.index[data_reset['Fish Type_encoded'] == fish_type_encoded].tolist()\n",
    "\n",
    "            if len(fish_positions) < self.config.sequence_length + self.config.horizon:\n",
    "                logger.warning(f\"Insufficient data for fish type {fish_type_encoded} ({len(fish_positions)} rows)\")\n",
    "                continue\n",
    "\n",
    "            for i in range(len(fish_positions) - self.config.sequence_length - self.config.horizon + 1):\n",
    "                seq_start_pos = i\n",
    "                seq_end_pos = seq_start_pos + self.config.sequence_length\n",
    "                target_pos = seq_end_pos + self.config.horizon - 1\n",
    "\n",
    "                seq_positions = fish_positions[seq_start_pos:seq_end_pos]\n",
    "                target_position = fish_positions[target_pos]\n",
    "\n",
    "                # Check date continuity\n",
    "                seq_dates = data_reset.loc[seq_positions, 'Date']\n",
    "                max_gap = timedelta(days=self.config.max_date_gap_days)\n",
    "                if len(seq_dates) > 1 and (seq_dates.diff().dropna() > max_gap).any():\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    X = feature_data[seq_positions]\n",
    "                    y = target_data[target_position]\n",
    "\n",
    "                    if np.any(y != 0) and not np.any(np.isnan(X)) and not np.any(np.isnan(y)):\n",
    "                        sequences.append({'features': X.astype(np.float32), 'targets': y.astype(np.float32)})\n",
    "\n",
    "                except (IndexError, ValueError) as e:\n",
    "                    logger.debug(f\"Skipping sequence due to error: {e}\")\n",
    "                    continue\n",
    "\n",
    "        logger.info(f\"Created {len(sequences)} sequences for {self.mode} dataset\")\n",
    "        return sequences\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        if idx >= len(self.sequences):\n",
    "            raise IndexError(f\"Index {idx} out of range for dataset of size {len(self.sequences)}\")\n",
    "\n",
    "        sequence = self.sequences[idx]\n",
    "        features = torch.from_numpy(sequence['features'])\n",
    "        targets = torch.from_numpy(sequence['targets'])\n",
    "\n",
    "        return features, targets\n",
    "\n",
    "\n",
    "class FishPriceNN(nn.Module):\n",
    "    \"\"\"Neural Network for fish price prediction\"\"\"\n",
    "\n",
    "    def __init__(self, input_size: int, config: Config):\n",
    "        super(FishPriceNN, self).__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=config.hidden_size,\n",
    "            num_layers=config.num_layers,\n",
    "            dropout=config.dropout if config.num_layers > 1 else 0,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.fc1 = nn.Linear(config.hidden_size, config.hidden_size // 2)\n",
    "        self.fc2 = nn.Linear(config.hidden_size // 2, len(config.target_columns))\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        last_output = lstm_out[:, -1, :]\n",
    "        output = self.dropout(last_output)\n",
    "        output = self.relu(self.fc1(output))\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc2(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "class FishPricePipeline:\n",
    "    \"\"\"Complete pipeline for fish price prediction\"\"\"\n",
    "\n",
    "    def __init__(self, config: Config):\n",
    "        if not config.validate():\n",
    "            raise ValueError(\"Invalid configuration\")\n",
    "\n",
    "        self.config = config\n",
    "        self.processor = FishPriceDataProcessor(config)\n",
    "        self.model = None\n",
    "        self.is_trained = False\n",
    "\n",
    "    def load_data(self, csv_path: str) -> pd.DataFrame:\n",
    "        \"\"\"Load data from CSV file\"\"\"\n",
    "        if not os.path.exists(csv_path):\n",
    "            raise FileNotFoundError(f\"CSV file not found: {csv_path}\")\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            logger.info(f\"Loaded data with shape: {df.shape}\")\n",
    "\n",
    "            # Basic validation\n",
    "            if df.empty:\n",
    "                raise ValueError(\"Loaded DataFrame is empty\")\n",
    "\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading CSV file: {e}\")\n",
    "            raise\n",
    "\n",
    "    def prepare_data(self, df: pd.DataFrame) -> Tuple[FishPriceDataset, FishPriceDataset, FishPriceDataset]:\n",
    "        \"\"\"Prepare train, validation, and test datasets\"\"\"\n",
    "        try:\n",
    "            logger.info(\"Processing data...\")\n",
    "            processed_df = self.processor.fit_transform(df)\n",
    "            processed_df = processed_df.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "            # Time-based split\n",
    "            unique_dates = sorted(processed_df['Date'].unique())\n",
    "            n_dates = len(unique_dates)\n",
    "\n",
    "            train_end_idx = int(n_dates * (1 - self.config.test_size - self.config.val_size))\n",
    "            val_end_idx = int(n_dates * (1 - self.config.test_size))\n",
    "\n",
    "            train_end_date = unique_dates[train_end_idx - 1]\n",
    "            val_end_date = unique_dates[val_end_idx - 1]\n",
    "\n",
    "            # Split datasets\n",
    "            train_df = processed_df[processed_df['Date'] <= train_end_date].copy()\n",
    "            val_df = processed_df[\n",
    "                (processed_df['Date'] > train_end_date) &\n",
    "                (processed_df['Date'] <= val_end_date)\n",
    "            ].copy()\n",
    "            test_df = processed_df[processed_df['Date'] > val_end_date].copy()\n",
    "\n",
    "            logger.info(f\"Data split - Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "            # Create datasets\n",
    "            train_dataset = FishPriceDataset(train_df, self.processor, self.config, 'train')\n",
    "            val_dataset = FishPriceDataset(val_df, self.processor, self.config, 'val')\n",
    "            test_dataset = FishPriceDataset(test_df, self.processor, self.config, 'test')\n",
    "\n",
    "            if len(train_dataset) == 0:\n",
    "                raise ValueError(\"No training sequences created\")\n",
    "            if len(val_dataset) == 0:\n",
    "                raise ValueError(\"No validation sequences created\")\n",
    "\n",
    "            return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error preparing data: {e}\")\n",
    "            raise\n",
    "\n",
    "    def train_neural_network(self, train_dataset: FishPriceDataset, val_dataset: FishPriceDataset):\n",
    "        \"\"\"Train neural network model\"\"\"\n",
    "        try:\n",
    "            train_loader = DataLoader(train_dataset, batch_size=self.config.batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=self.config.batch_size, shuffle=False)\n",
    "\n",
    "            input_size = len(self.processor.feature_columns)\n",
    "            self.model = FishPriceNN(input_size, self.config)\n",
    "\n",
    "            criterion = nn.MSELoss(reduction='none')\n",
    "            optimizer = optim.Adam(self.model.parameters(), lr=self.config.learning_rate)\n",
    "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "\n",
    "            best_val_loss = float('inf')\n",
    "            patience_counter = 0\n",
    "\n",
    "            logger.info(\"Starting neural network training...\")\n",
    "\n",
    "            for epoch in range(self.config.epochs):\n",
    "                # Training phase\n",
    "                self.model.train()\n",
    "                total_train_loss = 0\n",
    "                train_samples = 0\n",
    "\n",
    "                for batch_features, batch_targets in train_loader:\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = self.model(batch_features)\n",
    "\n",
    "                    loss = criterion(outputs, batch_targets)\n",
    "                    weights = (batch_targets != 0).float() + 0.1\n",
    "                    weighted_loss = (loss * weights).mean()\n",
    "\n",
    "                    weighted_loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    total_train_loss += weighted_loss.item() * batch_features.size(0)\n",
    "                    train_samples += batch_features.size(0)\n",
    "\n",
    "                # Validation phase\n",
    "                self.model.eval()\n",
    "                total_val_loss = 0\n",
    "                val_samples = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for batch_features, batch_targets in val_loader:\n",
    "                        outputs = self.model(batch_features)\n",
    "                        loss = criterion(outputs, batch_targets)\n",
    "                        weights = (batch_targets != 0).float() + 0.1\n",
    "                        weighted_loss = (loss * weights).mean()\n",
    "\n",
    "                        total_val_loss += weighted_loss.item() * batch_features.size(0)\n",
    "                        val_samples += batch_features.size(0)\n",
    "\n",
    "                avg_train_loss = total_train_loss / train_samples if train_samples > 0 else 0\n",
    "                avg_val_loss = total_val_loss / val_samples if val_samples > 0 else 0\n",
    "\n",
    "                scheduler.step(avg_val_loss)\n",
    "\n",
    "                if epoch % 10 == 0:\n",
    "                    logger.info(f\"Epoch {epoch}/{self.config.epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "                # Early stopping\n",
    "                if avg_val_loss < best_val_loss:\n",
    "                    best_val_loss = avg_val_loss\n",
    "                    patience_counter = 0\n",
    "                    torch.save(self.model.state_dict(), 'best_fish_price_model.pth')\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= self.config.patience:\n",
    "                        logger.info(f\"Early stopping at epoch {epoch}. Best Val Loss: {best_val_loss:.4f}\")\n",
    "                        break\n",
    "\n",
    "            # Load best model\n",
    "            if os.path.exists('best_fish_price_model.pth'):\n",
    "                self.model.load_state_dict(torch.load('best_fish_price_model.pth'))\n",
    "\n",
    "            self.is_trained = True\n",
    "            logger.info(\"Neural network training completed!\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error training neural network: {e}\")\n",
    "            raise\n",
    "\n",
    "    def train_random_forest(self, train_dataset: FishPriceDataset, val_dataset: FishPriceDataset):\n",
    "        \"\"\"Train Random Forest model\"\"\"\n",
    "        try:\n",
    "            logger.info(\"Preparing Random Forest features...\")\n",
    "\n",
    "            train_features, train_targets = self._extract_rf_features(train_dataset)\n",
    "\n",
    "            logger.info(f\"Random Forest feature shape: {train_features.shape}\")\n",
    "\n",
    "            self.model = {}\n",
    "            for i, target_name in enumerate(self.config.target_columns):\n",
    "                mask = train_targets[:, i] != 0\n",
    "\n",
    "                if np.sum(mask) < 10:\n",
    "                    logger.warning(f\"Insufficient non-zero samples for {target_name}\")\n",
    "                    continue\n",
    "\n",
    "                logger.info(f\"Training Random Forest for {target_name} with {np.sum(mask)} samples\")\n",
    "\n",
    "                rf = RandomForestRegressor(\n",
    "                    n_estimators=self.config.rf_n_estimators,\n",
    "                    max_depth=self.config.rf_max_depth,\n",
    "                    min_samples_split=self.config.rf_min_samples_split,\n",
    "                    min_samples_leaf=self.config.rf_min_samples_leaf,\n",
    "                    random_state=self.config.random_state,\n",
    "                    n_jobs=-1\n",
    "                )\n",
    "\n",
    "                rf.fit(train_features[mask], train_targets[mask, i])\n",
    "                self.model[target_name] = rf\n",
    "\n",
    "            self.is_trained = True\n",
    "            logger.info(\"Random Forest training completed!\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error training Random Forest: {e}\")\n",
    "            raise\n",
    "\n",
    "    def train_xgboost(self, train_dataset: FishPriceDataset, val_dataset: FishPriceDataset):\n",
    "        \"\"\"Train XGBoost model\"\"\"\n",
    "        try:\n",
    "            logger.info(\"Preparing XGBoost features...\")\n",
    "\n",
    "            train_features, train_targets = self._extract_xgb_features(train_dataset)\n",
    "            val_features, val_targets = self._extract_xgb_features(val_dataset)\n",
    "\n",
    "            logger.info(f\"XGBoost feature shape: {train_features.shape}\")\n",
    "\n",
    "            self.model = {}\n",
    "            for i, target_name in enumerate(self.config.target_columns):\n",
    "                train_mask = train_targets[:, i] != 0\n",
    "                val_mask = val_targets[:, i] != 0\n",
    "\n",
    "                if np.sum(train_mask) < 10:\n",
    "                    logger.warning(f\"Insufficient non-zero samples for {target_name}\")\n",
    "                    continue\n",
    "\n",
    "                logger.info(f\"Training XGBoost for {target_name} with {np.sum(train_mask)} training samples\")\n",
    "\n",
    "                dtrain = xgb.DMatrix(train_features[train_mask], label=train_targets[train_mask, i])\n",
    "\n",
    "                if np.sum(val_mask) > 0:\n",
    "                    dval = xgb.DMatrix(val_features[val_mask], label=val_targets[val_mask, i])\n",
    "                    evallist = [(dtrain, 'train'), (dval, 'val')]\n",
    "                else:\n",
    "                    evallist = [(dtrain, 'train')]\n",
    "\n",
    "                params = {\n",
    "                    'objective': 'reg:squarederror',\n",
    "                    'max_depth': self.config.xgb_max_depth,\n",
    "                    'learning_rate': self.config.xgb_learning_rate,\n",
    "                    'subsample': self.config.xgb_subsample,\n",
    "                    'colsample_bytree': self.config.xgb_colsample_bytree,\n",
    "                    'reg_alpha': self.config.xgb_reg_alpha,\n",
    "                    'reg_lambda': self.config.xgb_reg_lambda,\n",
    "                    'random_state': self.config.random_state,\n",
    "                    'verbosity': 0,\n",
    "                    'eval_metric': 'rmse'\n",
    "                }\n",
    "\n",
    "                xgb_model = xgb.train(\n",
    "                    params,\n",
    "                    dtrain,\n",
    "                    num_boost_round=self.config.xgb_n_estimators,\n",
    "                    evals=evallist,\n",
    "                    early_stopping_rounds=self.config.xgb_early_stopping_rounds,\n",
    "                    verbose_eval=False\n",
    "                )\n",
    "\n",
    "                self.model[target_name] = xgb_model\n",
    "\n",
    "            self.is_trained = True\n",
    "            logger.info(\"XGBoost training completed!\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error training XGBoost: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _extract_rf_features(self, dataset: FishPriceDataset) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Extract features for Random Forest\"\"\"\n",
    "        features, targets = [], []\n",
    "\n",
    "        for seq_features, seq_targets in dataset:\n",
    "            seq_data = seq_features.numpy()\n",
    "            seq_stats = np.concatenate([\n",
    "                seq_data.mean(axis=0),\n",
    "                seq_data.std(axis=0),\n",
    "                seq_data.max(axis=0),\n",
    "                seq_data.min(axis=0),\n",
    "                seq_data[-1, :],\n",
    "                seq_data[0, :],\n",
    "            ])\n",
    "\n",
    "            features.append(seq_stats)\n",
    "            targets.append(seq_targets.numpy())\n",
    "\n",
    "        return np.array(features), np.array(targets)\n",
    "\n",
    "    def _extract_xgb_features(self, dataset: FishPriceDataset) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Extract features for XGBoost\"\"\"\n",
    "        features, targets = [], []\n",
    "\n",
    "        for seq_features, seq_targets in dataset:\n",
    "            seq_data = seq_features.numpy()\n",
    "            seq_stats = np.concatenate([\n",
    "                seq_data.mean(axis=0),\n",
    "                seq_data.std(axis=0),\n",
    "                seq_data.max(axis=0),\n",
    "                seq_data.min(axis=0),\n",
    "                seq_data[-1, :],\n",
    "                seq_data[0, :],\n",
    "                np.median(seq_data, axis=0),\n",
    "                np.percentile(seq_data, 25, axis=0),\n",
    "                np.percentile(seq_data, 75, axis=0),\n",
    "            ])\n",
    "\n",
    "            features.append(seq_stats)\n",
    "            targets.append(seq_targets.numpy())\n",
    "\n",
    "        return np.array(features), np.array(targets)\n",
    "\n",
    "    def predict(self, test_dataset: FishPriceDataset) -> np.ndarray:\n",
    "        \"\"\"Make predictions on test dataset\"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Model not trained. Call train method first.\")\n",
    "\n",
    "        try:\n",
    "            if self.config.model_type == 'neural_network':\n",
    "                return self._predict_neural_network(test_dataset)\n",
    "            elif self.config.model_type == 'xgboost':\n",
    "                return self._predict_xgboost(test_dataset)\n",
    "            else:\n",
    "                return self._predict_random_forest(test_dataset)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error making predictions: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _predict_neural_network(self, test_dataset: FishPriceDataset) -> np.ndarray:\n",
    "        \"\"\"Predict using neural network\"\"\"\n",
    "        self.model.eval()\n",
    "        predictions = []\n",
    "        test_loader = DataLoader(test_dataset, batch_size=self.config.batch_size, shuffle=False)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for features, _ in test_loader:\n",
    "                outputs = self.model(features)\n",
    "                predictions.append(outputs.cpu().numpy())\n",
    "\n",
    "        return np.vstack(predictions) if predictions else np.array([])\n",
    "\n",
    "    def _predict_random_forest(self, test_dataset: FishPriceDataset) -> np.ndarray:\n",
    "        \"\"\"Predict using random forest\"\"\"\n",
    "        test_features, _ = self._extract_rf_features(test_dataset)\n",
    "\n",
    "        if len(test_features) == 0:\n",
    "            return np.array([])\n",
    "\n",
    "        predictions = np.zeros((len(test_features), len(self.config.target_columns)))\n",
    "\n",
    "        for i, target_name in enumerate(self.config.target_columns):\n",
    "            if target_name in self.model:\n",
    "                predictions[:, i] = self.model[target_name].predict(test_features)\n",
    "            else:\n",
    "                logger.warning(f\"No model found for {target_name}\")\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def _predict_xgboost(self, test_dataset: FishPriceDataset) -> np.ndarray:\n",
    "        \"\"\"Predict using XGBoost\"\"\"\n",
    "        test_features, _ = self._extract_xgb_features(test_dataset)\n",
    "\n",
    "        if len(test_features) == 0:\n",
    "            return np.array([])\n",
    "\n",
    "        predictions = np.zeros((len(test_features), len(self.config.target_columns)))\n",
    "        dtest = xgb.DMatrix(test_features)\n",
    "\n",
    "        for i, target_name in enumerate(self.config.target_columns):\n",
    "            if target_name in self.model:\n",
    "                predictions[:, i] = self.model[target_name].predict(dtest)\n",
    "            else:\n",
    "                logger.warning(f\"No model found for {target_name}\")\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def evaluate(self, test_dataset: FishPriceDataset) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate model performance\"\"\"\n",
    "        if len(test_dataset) == 0:\n",
    "            return {}\n",
    "\n",
    "        try:\n",
    "            predictions = self.predict(test_dataset)\n",
    "            if len(predictions) == 0:\n",
    "                return {}\n",
    "\n",
    "            actual_targets = np.array([t.numpy() for _, t in test_dataset])\n",
    "\n",
    "            metrics = {}\n",
    "            for i, target_name in enumerate(self.config.target_columns):\n",
    "                mask = actual_targets[:, i] != 0\n",
    "\n",
    "                if np.sum(mask) == 0:\n",
    "                    logger.warning(f\"No non-zero targets for {target_name}\")\n",
    "                    continue\n",
    "\n",
    "                y_true = actual_targets[mask, i]\n",
    "                y_pred = predictions[mask, i]\n",
    "\n",
    "                metrics[f'{target_name}_mse'] = mean_squared_error(y_true, y_pred)\n",
    "                metrics[f'{target_name}_rmse'] = np.sqrt(metrics[f'{target_name}_mse'])\n",
    "                metrics[f'{target_name}_mae'] = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "                try:\n",
    "                    metrics[f'{target_name}_r2'] = r2_score(y_true, y_pred)\n",
    "                except Exception:\n",
    "                    metrics[f'{target_name}_r2'] = float('nan')\n",
    "\n",
    "            return metrics\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error evaluating model: {e}\")\n",
    "            raise\n",
    "\n",
    "    def save_pipeline(self, path: str):\n",
    "        \"\"\"Save the complete pipeline\"\"\"\n",
    "        try:\n",
    "            pipeline_data = {\n",
    "                'config': self.config,\n",
    "                'processor': self.processor\n",
    "            }\n",
    "\n",
    "            with open(f'{path}_pipeline.pkl', 'wb') as f:\n",
    "                pickle.dump(pipeline_data, f)\n",
    "\n",
    "            if self.config.model_type == 'neural_network' and self.model is not None:\n",
    "                torch.save(self.model.state_dict(), f'{path}_model.pth')\n",
    "            elif self.config.model_type in ['random_forest', 'xgboost'] and self.model is not None:\n",
    "                with open(f'{path}_model.pkl', 'wb') as f:\n",
    "                    pickle.dump(self.model, f)\n",
    "\n",
    "            logger.info(f\"Pipeline and model saved to {path}_...\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving pipeline: {e}\")\n",
    "            raise\n",
    "\n",
    "    def load_pipeline(self, path: str):\n",
    "        \"\"\"Load a saved pipeline\"\"\"\n",
    "        try:\n",
    "            with open(f'{path}_pipeline.pkl', 'rb') as f:\n",
    "                pipeline_data = pickle.load(f)\n",
    "\n",
    "            self.config = pipeline_data['config']\n",
    "            self.processor = pipeline_data['processor']\n",
    "\n",
    "            if self.config.model_type == 'neural_network':\n",
    "                input_size = len(self.processor.feature_columns)\n",
    "                self.model = FishPriceNN(input_size, self.config)\n",
    "                self.model.load_state_dict(torch.load(f'{path}_model.pth'))\n",
    "                self.model.eval()\n",
    "            else:\n",
    "                with open(f'{path}_model.pkl', 'rb') as f:\n",
    "                    self.model = pickle.load(f)\n",
    "\n",
    "            self.is_trained = True\n",
    "            logger.info(f\"Pipeline loaded from {path}_...\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading pipeline: {e}\")\n",
    "            raise\n",
    "\n",
    "    def predict_future(self, df: pd.DataFrame, fish_type: str, n_days: int = 1) -> Dict[str, List[float]]:\n",
    "        \"\"\"Predict future prices for a specific fish type\"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Model not trained. Call train method first.\")\n",
    "\n",
    "        try:\n",
    "            processed_df = self.processor.transform(df)\n",
    "            fish_data = processed_df[processed_df['Fish Type'] == fish_type].copy()\n",
    "            fish_data = fish_data.sort_values('Date').tail(self.config.sequence_length)\n",
    "\n",
    "            if len(fish_data) < self.config.sequence_length:\n",
    "                raise ValueError(f\"Insufficient data for {fish_type}. Need at least {self.config.sequence_length} days.\")\n",
    "\n",
    "            predictions = {target: [] for target in self.config.target_columns}\n",
    "            current_sequence = fish_data[self.processor.feature_columns].values\n",
    "\n",
    "            for day in range(n_days):\n",
    "                if self.config.model_type == 'neural_network':\n",
    "                    seq_tensor = torch.from_numpy(current_sequence.astype(np.float32)).unsqueeze(0)\n",
    "                    self.model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        pred = self.model(seq_tensor).cpu().numpy()[0]\n",
    "                else:\n",
    "                    if self.config.model_type == 'xgboost':\n",
    "                        seq_stats = np.concatenate([\n",
    "                            current_sequence.mean(axis=0),\n",
    "                            current_sequence.std(axis=0),\n",
    "                            current_sequence.max(axis=0),\n",
    "                            current_sequence.min(axis=0),\n",
    "                            current_sequence[-1, :],\n",
    "                            current_sequence[0, :],\n",
    "                            np.median(current_sequence, axis=0),\n",
    "                            np.percentile(current_sequence, 25, axis=0),\n",
    "                            np.percentile(current_sequence, 75, axis=0),\n",
    "                        ]).reshape(1, -1)\n",
    "                    else:\n",
    "                        seq_stats = np.concatenate([\n",
    "                            current_sequence.mean(axis=0),\n",
    "                            current_sequence.std(axis=0),\n",
    "                            current_sequence.max(axis=0),\n",
    "                            current_sequence.min(axis=0),\n",
    "                            current_sequence[-1, :],\n",
    "                            current_sequence[0, :],\n",
    "                        ]).reshape(1, -1)\n",
    "\n",
    "                    pred = np.zeros(len(self.config.target_columns))\n",
    "                    for i, target_name in enumerate(self.config.target_columns):\n",
    "                        if target_name in self.model:\n",
    "                            if self.config.model_type == 'xgboost':\n",
    "                                dtest = xgb.DMatrix(seq_stats)\n",
    "                                pred[i] = self.model[target_name].predict(dtest)[0]\n",
    "                            else:\n",
    "                                pred[i] = self.model[target_name].predict(seq_stats)[0]\n",
    "\n",
    "                for i, target in enumerate(self.config.target_columns):\n",
    "                    predictions[target].append(max(0, pred[i]))\n",
    "\n",
    "                current_sequence = np.roll(current_sequence, -1, axis=0)\n",
    "                current_sequence[-1] = current_sequence[-2]\n",
    "\n",
    "            return predictions\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error predicting future prices: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "def print_model_specific_usage(model_type: str):\n",
    "    \"\"\"Print model-specific usage instructions\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"FISH PRICE PREDICTION PIPELINE - {model_type.upper().replace('_', ' ')} MODEL\")\n",
    "    print(\"=\"*60)\n",
    "    print()\n",
    "\n",
    "    if model_type == 'neural_network':\n",
    "        print(\"NEURAL NETWORK MODEL FEATURES:\")\n",
    "        print(\"- LSTM-based deep learning architecture\")\n",
    "        print(\"- Handles sequential patterns in time series data\")\n",
    "        print(\"- Automatic feature learning from raw sequences\")\n",
    "        print(\"- Early stopping and learning rate scheduling\")\n",
    "        print(\"- Best for: Complex temporal patterns, large datasets\")\n",
    "        print()\n",
    "        print(\"KEY PARAMETERS (configurable in Config class):\")\n",
    "        print(\"- hidden_size: LSTM hidden units (default: 128)\")\n",
    "        print(\"- num_layers: Number of LSTM layers (default: 3)\")\n",
    "        print(\"- dropout: Dropout rate for regularization (default: 0.2)\")\n",
    "        print(\"- learning_rate: Adam optimizer learning rate (default: 0.001)\")\n",
    "        print(\"- batch_size: Training batch size (default: 32)\")\n",
    "        print(\"- epochs: Maximum training epochs (default: 100)\")\n",
    "        print(\"- patience: Early stopping patience (default: 10)\")\n",
    "\n",
    "    elif model_type == 'random_forest':\n",
    "        print(\"RANDOM FOREST MODEL FEATURES:\")\n",
    "        print(\"- Ensemble of decision trees\")\n",
    "        print(\"- Robust to overfitting\")\n",
    "        print(\"- Feature importance analysis\")\n",
    "        print(\"- No hyperparameter tuning required\")\n",
    "        print(\"- Best for: Interpretable results, medium datasets\")\n",
    "        print()\n",
    "        print(\"KEY PARAMETERS (configurable in Config class):\")\n",
    "        print(\"- rf_n_estimators: Number of trees (default: 100)\")\n",
    "        print(\"- rf_max_depth: Maximum tree depth (default: 20)\")\n",
    "        print(\"- rf_min_samples_split: Min samples to split (default: 5)\")\n",
    "        print(\"- rf_min_samples_leaf: Min samples per leaf (default: 2)\")\n",
    "\n",
    "    elif model_type == 'xgboost':\n",
    "        print(\"XGBOOST MODEL FEATURES:\")\n",
    "        print(\"- Gradient boosting with advanced optimization\")\n",
    "        print(\"- Built-in regularization (L1/L2)\")\n",
    "        print(\"- Early stopping with validation monitoring\")\n",
    "        print(\"- Memory efficient and fast training\")\n",
    "        print(\"- Best for: High performance, structured data\")\n",
    "        print()\n",
    "        print(\"KEY PARAMETERS (configurable in Config class):\")\n",
    "        print(\"- xgb_n_estimators: Boosting rounds (default: 100)\")\n",
    "        print(\"- xgb_max_depth: Maximum tree depth (default: 6)\")\n",
    "        print(\"- xgb_learning_rate: Learning rate (default: 0.1)\")\n",
    "        print(\"- xgb_subsample: Row subsampling (default: 0.8)\")\n",
    "        print(\"- xgb_colsample_bytree: Column subsampling (default: 0.8)\")\n",
    "        print(\"- xgb_reg_alpha: L1 regularization (default: 0.1)\")\n",
    "        print(\"- xgb_reg_lambda: L2 regularization (default: 1.0)\")\n",
    "        print(\"- xgb_early_stopping_rounds: Early stopping patience (default: 10)\")\n",
    "\n",
    "    print()\n",
    "    print(\"USAGE EXAMPLES:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"1. Train and save model:\")\n",
    "    print(\"   python fish_price_pipeline.py\")\n",
    "    print()\n",
    "    print(\"2. Load and make predictions:\")\n",
    "    print(\"   predictions = load_and_predict(\")\n",
    "    print(\"       pipeline_path='fish_price_pipeline',\")\n",
    "    print(\"       data_path='your_data.csv',\")\n",
    "    print(\"       fish_type='Skipjack',\")\n",
    "    print(\"       n_days=7\")\n",
    "    print(\"   )\")\n",
    "    print()\n",
    "    print(\"3. Change model configuration:\")\n",
    "    print(\"   config = Config()\")\n",
    "    print(f\"   config.model_type = '{model_type}'\")\n",
    "    if model_type == 'neural_network':\n",
    "        print(\"   config.hidden_size = 256  # Increase model capacity\")\n",
    "        print(\"   config.learning_rate = 0.0005  # Fine-tune learning\")\n",
    "    elif model_type == 'random_forest':\n",
    "        print(\"   config.rf_n_estimators = 200  # More trees\")\n",
    "        print(\"   config.rf_max_depth = 30  # Deeper trees\")\n",
    "    elif model_type == 'xgboost':\n",
    "        print(\"   config.xgb_learning_rate = 0.05  # Slower learning\")\n",
    "        print(\"   config.xgb_n_estimators = 200  # More boosting rounds\")\n",
    "\n",
    "    print()\n",
    "    print(\"FILES CREATED:\")\n",
    "    print(\"- fish_price_pipeline_pipeline.pkl (preprocessor + config)\")\n",
    "    if model_type == 'neural_network':\n",
    "        print(\"- fish_price_pipeline_model.pth (trained neural network)\")\n",
    "        print(\"- best_fish_price_model.pth (best model checkpoint)\")\n",
    "    else:\n",
    "        print(\"- fish_price_pipeline_model.pkl (trained model)\")\n",
    "\n",
    "    print()\n",
    "    print(\"LOG FILES:\")\n",
    "    print(\"- fish_price_pipeline.log (detailed execution log)\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the fish price prediction pipeline\"\"\"\n",
    "    try:\n",
    "        config = Config()\n",
    "\n",
    "        # Validate configuration\n",
    "        if not config.validate():\n",
    "            logger.error(\"Invalid configuration. Please check your settings.\")\n",
    "            return False\n",
    "\n",
    "        logger.info(\"Fish Price Prediction Pipeline Starting...\")\n",
    "        logger.info(f\"Model Type: {config.model_type}\")\n",
    "        logger.info(f\"Sequence Length: {config.sequence_length} days\")\n",
    "        logger.info(f\"Prediction Horizon: {config.horizon} day(s)\")\n",
    "\n",
    "        pipeline = FishPricePipeline(config)\n",
    "\n",
    "        # Load data\n",
    "        csv_path = 'Final data set 2025 08 10.csv'  # Update this path\n",
    "\n",
    "        try:\n",
    "            df = pipeline.load_data(csv_path)\n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"CSV file not found: {csv_path}\")\n",
    "            print(f\"Error: '{csv_path}' not found.\")\n",
    "            print(\"Please update the csv_path variable with the correct file path.\")\n",
    "            return False\n",
    "\n",
    "        # Prepare datasets\n",
    "        logger.info(\"Preparing datasets...\")\n",
    "        train_dataset, val_dataset, test_dataset = pipeline.prepare_data(df)\n",
    "        logger.info(f\"Dataset sizes: Train={len(train_dataset)}, Val={len(val_dataset)}, Test={len(test_dataset)}\")\n",
    "\n",
    "        if len(train_dataset) == 0:\n",
    "            logger.error(\"No training data available\")\n",
    "            return False\n",
    "\n",
    "        # Train model\n",
    "        logger.info(f\"Training {config.model_type.replace('_', ' ').title()} model...\")\n",
    "\n",
    "        if config.model_type == 'neural_network':\n",
    "            pipeline.train_neural_network(train_dataset, val_dataset)\n",
    "        elif config.model_type == 'xgboost':\n",
    "            pipeline.train_xgboost(train_dataset, val_dataset)\n",
    "        else:\n",
    "            pipeline.train_random_forest(train_dataset, val_dataset)\n",
    "\n",
    "        # Evaluate model\n",
    "        if len(test_dataset) > 0:\n",
    "            logger.info(\"Evaluating model performance...\")\n",
    "            metrics = pipeline.evaluate(test_dataset)\n",
    "\n",
    "            if metrics:\n",
    "                logger.info(\"Model Performance Metrics:\")\n",
    "                for metric, value in metrics.items():\n",
    "                    if not np.isnan(value):\n",
    "                        logger.info(f\"  {metric}: {value:.4f}\")\n",
    "                    else:\n",
    "                        logger.info(f\"  {metric}: N/A\")\n",
    "\n",
    "                print(\"\\nMODEL PERFORMANCE METRICS:\")\n",
    "                print(\"-\" * 40)\n",
    "                for metric, value in metrics.items():\n",
    "                    if not np.isnan(value):\n",
    "                        print(f\"  {metric}: {value:.4f}\")\n",
    "                    else:\n",
    "                        print(f\"  {metric}: N/A\")\n",
    "            else:\n",
    "                logger.warning(\"No evaluation metrics available\")\n",
    "        else:\n",
    "            logger.warning(\"No test data available for evaluation\")\n",
    "\n",
    "        # Save pipeline\n",
    "        logger.info(\"Saving pipeline...\")\n",
    "        pipeline.save_pipeline('fish_price_pipeline')\n",
    "\n",
    "        # Example prediction\n",
    "        try:\n",
    "            fish_types = df['Fish Type'].unique()\n",
    "            if len(fish_types) > 0:\n",
    "                example_fish = fish_types[0]\n",
    "                logger.info(f\"Example prediction for {example_fish}:\")\n",
    "                future_predictions = pipeline.predict_future(df, example_fish, n_days=7)\n",
    "\n",
    "                print(f\"\\nEXAMPLE PREDICTION FOR {example_fish.upper()}:\")\n",
    "                print(\"-\" * 50)\n",
    "                for target, preds in future_predictions.items():\n",
    "                    print(f\"  {target}: {[f'{p:.2f}' for p in preds]}\")\n",
    "\n",
    "                for target, preds in future_predictions.items():\n",
    "                    logger.info(f\"  {target}: {preds}\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Example prediction failed: {e}\")\n",
    "\n",
    "        logger.info(\"Fish Price Prediction Pipeline completed successfully!\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Pipeline failed with error: {e}\", exc_info=True)\n",
    "        return False\n",
    "\n",
    "\n",
    "def load_and_predict(pipeline_path: str, data_path: str, fish_type: str, n_days: int = 7) -> Optional[Dict[str, List[float]]]:\n",
    "    \"\"\"Utility function to load a saved pipeline and make predictions\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Loading pipeline from {pipeline_path}\")\n",
    "\n",
    "        config = Config()\n",
    "        pipeline = FishPricePipeline(config)\n",
    "        pipeline.load_pipeline(pipeline_path)\n",
    "\n",
    "        logger.info(f\"Loading data from {data_path}\")\n",
    "        df = pipeline.load_data(data_path)\n",
    "\n",
    "        logger.info(f\"Making predictions for {fish_type} over {n_days} days\")\n",
    "        predictions = pipeline.predict_future(df, fish_type, n_days)\n",
    "\n",
    "        print(f\"\\nPREDICTIONS FOR {fish_type.upper()} OVER NEXT {n_days} DAYS:\")\n",
    "        print(\"-\" * 60)\n",
    "        for target, preds in predictions.items():\n",
    "            print(f\"  {target}: {[f'{p:.2f}' for p in preds]}\")\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Prediction failed: {e}\", exc_info=True)\n",
    "        return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    success = main()\n",
    "\n",
    "    if success:\n",
    "        # Get the model type from config to show appropriate usage\n",
    "        config = Config()  # This will have the default model_type\n",
    "        print_model_specific_usage(config.model_type)\n",
    "    else:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"PIPELINE EXECUTION FAILED\")\n",
    "        print(\"=\"*50)\n",
    "        print(\"Please check the log file 'fish_price_pipeline.log' for detailed error information.\")\n",
    "        print()\n",
    "        print(\"Common issues and solutions:\")\n",
    "        print(\"1. Data file not found - Update the csv_path variable\")\n",
    "        print(\"2. Missing dependencies - Install required packages:\")\n",
    "        print(\"   pip install torch scikit-learn xgboost pandas numpy\")\n",
    "        print(\"3. Data format issues - Ensure CSV has required columns:\")\n",
    "        print(\"   ['Date', 'Fish Type', 'avg_ws_price', 'avg_rt_price']\")\n",
    "        print(\"4. Insufficient data - Ensure dataset has enough historical data\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
